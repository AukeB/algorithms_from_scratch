{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Tokenizer\n",
    "\n",
    "Auke Bruinsma\n",
    "\n",
    "Based on the video by Andrej Karpathy: https://www.youtube.com/watch?v=zduSFxRajkE&"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"A class for loading and cleaning text from a file.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader with a file path.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path to the text file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_text(self) -> str:\n",
    "        \"\"\"\n",
    "        Loads text from the specified file.\n",
    "\n",
    "        Returns:\n",
    "            (str): The contents of the file as a string, or an empty string if\n",
    "            an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                return file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{self.file_path}' not found.\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans the given text by removing extra spaces and newlines.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            (str): The cleaned text with excess spaces and newlines removed.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\" {2,}\", \" \", text)\n",
    "        text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =\"data/herman_finkers.txt\"\n",
    "\n",
    "data_loader = DataLoader(file_path)\n",
    "text = data_loader.load_text()\n",
    "cleaned_text = data_loader.clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'De beginnend cabaret'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = cleaned_text.encode(\"utf-8\")\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68, 101, 32, 98, 101, 103, 105, 110, 110, 101, 110, 100, 32, 99, 97, 98, 97, 114, 101, 116]\n"
     ]
    }
   ],
   "source": [
    "tokens = list(map(int, tokens))\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "Find the pair of bytes that occur most frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(110, 32)]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_top_pairs(encoded_text: list[int]) -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Finds the most frequently occurring adjacent byte pairs.\n",
    "\n",
    "    Args:\n",
    "        encoded_text (list[int]): A list of integers representing encoded text.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int, int]]: A list of the most frequent byte pairs.\n",
    "    \"\"\"\n",
    "    byte_pair_count = defaultdict(int)\n",
    "\n",
    "    for current_element, next_element in zip(encoded_text[:-2], encoded_text[1:]):\n",
    "        byte_pair = (current_element, next_element)\n",
    "        byte_pair_count[byte_pair] += 1\n",
    "    \n",
    "    highest_count = max(byte_pair_count.values(), default=0)\n",
    "    top_pairs = [byte_pair for byte_pair, count in byte_pair_count.items() if count == highest_count]\n",
    "\n",
    "    return top_pairs\n",
    "\n",
    "top_pairs = find_top_pairs(encoded_text=tokens)\n",
    "top_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(140, (110, 32)), (136, (101, 110)), (98, (101, 114)), (86, (116, 32)), (74, (32, 105)), (73, (107, 32)), (73, (101, 101)), (71, (101, 32)), (68, (32, 101)), (68, (32, 100)), (65, (100, 101)), (58, (115, 32)), (57, (114, 32)), (51, (105, 106)), (50, (32, 119)), (49, (44, 32)), (48, (32, 104)), (46, (105, 101)), (46, (101, 108)), (46, (97, 110)), (45, (105, 110)), (45, (101, 116)), (45, (97, 97)), (45, (32, 109)), (43, (105, 107)), (42, (103, 101)), (40, (104, 101)), (40, (32, 111)), (37, (32, 97)), (35, (108, 32)), (34, (97, 108)), (33, (32, 118)), (32, (32, 122)), (31, (110, 100)), (30, (97, 114)), (29, (32, 110)), (28, (226, 128)), (28, (115, 116)), (28, (111, 111)), (28, (98, 101)), (28, (46, 32)), (27, (100, 97)), (27, (32, 98)), (25, (100, 32)), (25, (97, 116)), (25, (32, 103)), (24, (111, 110)), (24, (107, 101)), (23, (114, 101)), (23, (109, 105)), (23, (105, 115)), (23, (32, 107)), (22, (119, 101)), (22, (118, 101)), (22, (111, 114)), (22, (108, 101)), (22, (103, 32)), (22, (101, 115)), (21, (109, 101)), (20, (109, 97)), (20, (106, 32)), (19, (122, 101)), (19, (116, 101)), (19, (99, 104)), (18, (110, 105)), (18, (106, 101)), (18, (101, 103)), (18, (32, 115)), (18, (32, 10)), (18, (10, 32)), (17, (110, 115)), (16, (128, 153)), (16, (108, 105)), (16, (32, 108)), (15, (111, 101)), (15, (106, 110)), (15, (101, 105)), (14, (119, 97)), (14, (111, 112)), (14, (110, 116)), (14, (46, 10)), (13, (122, 111)), (13, (119, 105)), (13, (114, 100)), (13, (110, 103)), (13, (101, 100)), (13, (32, 116)), (12, (128, 152)), (12, (114, 111)), (12, (112, 32)), (12, (111, 103)), (12, (107, 111)), (12, (106, 107)), (12, (104, 97)), (12, (101, 98)), (11, (117, 107)), (11, (116, 115)), (11, (116, 105)), (11, (114, 97)), (11, (110, 111)), (11, (110, 46)), (11, (108, 97)), (11, (105, 103)), (11, (58, 32)), (10, (117, 105)), (10, (117, 32)), (10, (114, 115)), (10, (110, 107)), (10, (110, 101)), (10, (108, 115)), (10, (108, 108)), (10, (108, 100)), (10, (105, 116)), (10, (101, 117)), (10, (100, 105)), (9, (118, 97)), (9, (114, 116)), (9, (114, 98)), (9, (112, 101)), (9, (111, 107)), (9, (111, 32)), (9, (105, 100)), (9, (101, 109)), (9, (98, 105)), (9, (97, 109)), (9, (97, 99)), (8, (119, 111)), (8, (118, 111)), (8, (117, 115)), (8, (116, 106)), (8, (115, 112)), (8, (114, 107)), (8, (114, 105)), (8, (112, 114)), (8, (111, 118)), (8, (111, 109)), (8, (110, 44)), (8, (109, 111)), (8, (98, 32)), (8, (32, 226)), (8, (32, 117)), (8, (32, 68)), (7, (153, 10)), (7, (118, 114)), (7, (116, 114)), (7, (116, 111)), (7, (114, 44)), (7, (111, 117)), (7, (110, 97)), (7, (105, 108)), (7, (104, 116)), (7, (101, 107)), (7, (97, 44)), (7, (74, 97)), (7, (73, 107)), (7, (32, 99)), (6, (115, 46)), (6, (115, 44)), (6, (114, 103)), (6, (109, 32)), (6, (107, 105)), (6, (103, 97)), (6, (102, 105)), (6, (101, 122)), (6, (100, 114)), (6, (97, 103)), (6, (32, 102)), (5, (153, 32)), (5, (122, 105)), (5, (117, 119)), (5, (116, 97)), (5, (115, 101)), (5, (114, 46)), (5, (110, 110)), (5, (104, 117)), (5, (104, 111)), (5, (104, 32)), (5, (103, 105)), (5, (101, 118)), (5, (101, 112)), (5, (101, 46)), (5, (100, 117)), (5, (100, 46)), (5, (100, 44)), (5, (98, 97)), (5, (97, 100)), (5, (97, 98)), (5, (90, 111)), (5, (69, 110)), (5, (68, 117)), (5, (32, 106)), (5, (10, 226)), (4, (195, 169)), (4, (119, 32)), (4, (118, 105)), (4, (117, 114)), (4, (116, 46)), (4, (116, 44)), (4, (115, 99)), (4, (114, 108)), (4, (111, 108)), (4, (111, 99)), (4, (110, 99)), (4, (109, 116)), (4, (108, 117)), (4, (108, 116)), (4, (108, 111)), (4, (107, 108)), (4, (103, 114)), (4, (100, 100)), (4, (99, 97)), (4, (97, 107)), (4, (78, 101)), (4, (46, 226)), (4, (46, 46)), (4, (32, 112)), (4, (32, 90)), (4, (32, 86)), (4, (32, 78)), (4, (32, 74)), (4, (32, 73)), (4, (32, 71)), (3, (152, 105)), (3, (122, 97)), (3, (117, 109)), (3, (116, 119)), (3, (116, 117)), (3, (115, 106)), (3, (114, 109)), (3, (114, 104)), (3, (112, 116)), (3, (111, 97)), (3, (110, 109)), (3, (110, 108)), (3, (108, 112)), (3, (108, 102)), (3, (108, 44)), (3, (107, 107)), (3, (107, 44)), (3, (105, 32)), (3, (103, 116)), (3, (103, 100)), (3, (103, 46)), (3, (102, 32)), (3, (101, 119)), (3, (101, 104)), (3, (101, 58)), (3, (100, 58)), (3, (99, 101)), (3, (98, 98)), (3, (97, 117)), (3, (97, 115)), (3, (87, 97)), (3, (86, 111)), (3, (78, 111)), (3, (77, 105)), (3, (72, 101)), (3, (70, 105)), (3, (68, 101)), (3, (63, 226)), (3, (58, 10)), (3, (44, 226)), (3, (33, 32)), (3, (32, 195)), (3, (32, 87)), (3, (32, 83)), (3, (32, 70)), (3, (32, 69)), (3, (10, 78)), (3, (10, 77)), (3, (10, 73)), (3, (10, 69)), (2, (195, 188)), (2, (195, 171)), (2, (171, 110)), (2, (169, 195)), (2, (169, 110)), (2, (152, 74)), (2, (117, 116)), (2, (117, 110)), (2, (117, 108)), (2, (117, 100)), (2, (116, 10)), (2, (115, 108)), (2, (115, 105)), (2, (115, 58)), (2, (114, 117)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 115)), (2, (111, 102)), (2, (111, 44)), (2, (110, 117)), (2, (110, 58)), (2, (110, 33)), (2, (109, 109)), (2, (107, 117)), (2, (107, 97)), (2, (107, 46)), (2, (107, 33)), (2, (106, 111)), (2, (106, 108)), (2, (105, 104)), (2, (104, 114)), (2, (103, 111)), (2, (103, 103)), (2, (103, 63)), (2, (102, 109)), (2, (101, 195)), (2, (101, 44)), (2, (100, 116)), (2, (100, 106)), (2, (99, 111)), (2, (98, 117)), (2, (98, 114)), (2, (98, 108)), (2, (97, 112)), (2, (97, 106)), (2, (97, 105)), (2, (84, 101)), (2, (77, 97)), (2, (71, 114)), (2, (68, 97)), (2, (33, 226)), (2, (32, 114)), (2, (32, 84)), (2, (32, 80)), (2, (32, 79)), (2, (32, 75)), (2, (32, 72)), (2, (10, 74)), (2, (10, 68)), (1, (195, 179)), (1, (188, 99)), (1, (188, 98)), (1, (179, 32)), (1, (153, 116)), (1, (153, 115)), (1, (153, 110)), (1, (153, 106)), (1, (152, 90)), (1, (152, 87)), (1, (152, 78)), (1, (152, 77)), (1, (152, 76)), (1, (152, 72)), (1, (152, 69)), (1, (122, 195)), (1, (122, 117)), (1, (121, 110)), (1, (120, 116)), (1, (120, 101)), (1, (119, 117)), (1, (119, 99)), (1, (119, 98)), (1, (119, 46)), (1, (117, 122)), (1, (117, 120)), (1, (117, 117)), (1, (117, 112)), (1, (117, 98)), (1, (117, 44)), (1, (116, 195)), (1, (116, 109)), (1, (116, 104)), (1, (116, 58)), (1, (116, 33)), (1, (115, 117)), (1, (115, 115)), (1, (115, 111)), (1, (115, 104)), (1, (115, 97)), (1, (114, 122)), (1, (114, 119)), (1, (114, 118)), (1, (114, 58)), (1, (114, 10)), (1, (112, 103)), (1, (112, 44)), (1, (111, 226)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 105)), (1, (111, 100)), (1, (111, 98)), (1, (110, 98)), (1, (110, 63)), (1, (109, 117)), (1, (109, 115)), (1, (109, 112)), (1, (109, 100)), (1, (108, 109)), (1, (108, 103)), (1, (108, 63)), (1, (108, 46)), (1, (108, 10)), (1, (107, 122)), (1, (107, 119)), (1, (107, 116)), (1, (107, 115)), (1, (107, 99)), (1, (107, 58)), (1, (106, 119)), (1, (106, 103)), (1, (106, 100)), (1, (105, 114)), (1, (105, 99)), (1, (105, 97)), (1, (105, 58)), (1, (105, 44)), (1, (104, 105)), (1, (103, 115)), (1, (103, 44)), (1, (102, 101)), (1, (102, 97)), (1, (102, 46)), (1, (101, 226)), (1, (101, 120)), (1, (101, 102)), (1, (101, 99)), (1, (101, 63)), (1, (101, 33)), (1, (100, 118)), (1, (100, 107)), (1, (100, 10)), (1, (99, 116)), (1, (99, 108)), (1, (99, 107)), (1, (99, 105)), (1, (99, 46)), (1, (98, 111)), (1, (98, 46)), (1, (97, 226)), (1, (97, 118)), (1, (97, 102)), (1, (97, 32)), (1, (87, 111)), (1, (87, 105)), (1, (86, 101)), (1, (86, 97)), (1, (83, 117)), (1, (83, 116)), (1, (83, 105)), (1, (80, 105)), (1, (80, 101)), (1, (79, 118)), (1, (79, 109)), (1, (79, 102)), (1, (78, 105)), (1, (76, 101)), (1, (76, 97)), (1, (75, 105)), (1, (75, 101)), (1, (74, 101)), (1, (72, 117)), (1, (71, 121)), (1, (71, 111)), (1, (71, 101)), (1, (69, 101)), (1, (69, 100)), (1, (68, 105)), (1, (66, 111)), (1, (65, 108)), (1, (63, 32)), (1, (33, 44)), (1, (32, 77)), (1, (32, 66)), (1, (32, 65)), (1, (10, 109)), (1, (10, 97)), (1, (10, 87)), (1, (10, 86)), (1, (10, 79)), (1, (10, 76)), (1, (10, 72)), (1, (10, 71))]\n"
     ]
    }
   ],
   "source": [
    "stats = get_stats(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My new solution\n",
    "\n",
    "- Changed the for-loop to a more pythonic way to do it.\n",
    "- Changed method for retrieving max value of dictionary values, this one looks more nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 32)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_top_pair(encoded_text: list[int]) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Finds the most frequently occurring adjacent byte pair.\n",
    "\n",
    "    Args:\n",
    "        encoded_text (list[int]): A list of integers representing encoded text.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int]: The most frequent byte pair.\n",
    "    \"\"\"\n",
    "    byte_pair_count = defaultdict(int)\n",
    "\n",
    "    for current_element, next_element in zip(encoded_text, encoded_text[1:]):\n",
    "        byte_pair = (current_element, next_element)\n",
    "        byte_pair_count[byte_pair] += 1\n",
    "\n",
    "    return max(byte_pair_count, key=byte_pair_count.get, default=(0, 0))\n",
    "\n",
    "top_pair = find_top_pair(encoded_text=tokens)\n",
    "top_pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Merge all top pairs with a new byte value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_top_pair(\n",
    "    encoded_text: list[int],\n",
    "    top_pair_value: tuple[int, int],\n",
    "    new_byte_value: int = 256,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Replaces occurrences of a given byte pair in the encoded text.\n",
    "\n",
    "    This function scans the encoded text for consecutive occurrences of \n",
    "    `top_pair_value` and replaces them with `new_byte_value`.\n",
    "\n",
    "    Args:\n",
    "        encoded_text (list[int]): The input list of byte values.\n",
    "        top_pair_value (tuple[int, int]): The byte pair to be merged.\n",
    "        new_byte_value (int, optional): The replacement byte. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: The modified list with merged byte pairs.\n",
    "    \"\"\"\n",
    "    i = 0 # For tracking iterations\n",
    "    new_bytes = []\n",
    "\n",
    "    while i < len(encoded_text) - 1:\n",
    "        current_element = encoded_text[i]\n",
    "        next_element = encoded_text[i+1]\n",
    "\n",
    "        if current_element == top_pair_value[0] and next_element == top_pair_value[1]:\n",
    "            new_bytes.append(new_byte_value)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_bytes.append(current_element)\n",
    "            i += 1\n",
    "    \n",
    "    # If the last element was not part of a skipped pair, add it\n",
    "    if i < len(encoded_text):\n",
    "        new_bytes.append(encoded_text[i])\n",
    "    \n",
    "    return new_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print(merge_top_pair([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens)=4379\n",
      "len(new_encoded_text)=4239\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens)=}\")\n",
    "\n",
    "new_encoded_text = merge_top_pair(\n",
    "    encoded_text=tokens,\n",
    "    top_pair_value=top_pair,\n",
    ")\n",
    "\n",
    "print(f\"{len(new_encoded_text)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    \n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My new solution\n",
    "\n",
    "- Merged the while and if statement into a while statement containing the if statement, looks more nice and you handle the edge case inside the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_top_pair(\n",
    "    encoded_text: list[int],\n",
    "    top_pair_value: tuple[int, int],\n",
    "    new_byte_value: int = 256,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Replaces occurrences of a given byte pair in the encoded text.\n",
    "\n",
    "    This function scans the encoded text for consecutive occurrences of \n",
    "    `top_pair_value` and replaces them with `new_byte_value`.\n",
    "\n",
    "    Args:\n",
    "        encoded_text (list[int]): The input list of byte values.\n",
    "        top_pair_value (tuple[int, int]): The byte pair to be merged.\n",
    "        new_byte_value (int, optional): The replacement byte. Defaults to 256.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: The modified list with merged byte pairs.\n",
    "    \"\"\"\n",
    "    i = 0 # Iterations\n",
    "    new_bytes = []\n",
    "\n",
    "    while i < len(encoded_text):\n",
    "        if i < len(encoded_text) - 1 and (encoded_text[i] == top_pair_value[0] and encoded_text[i+1] == top_pair_value[1]):\n",
    "            new_bytes.append(new_byte_value)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_bytes.append(encoded_text[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print(merge_top_pair([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens)=4379\n",
      "len(new_encoded_text)=4239\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens)=}\")\n",
    "\n",
    "new_encoded_text = merge_top_pair(\n",
    "    encoded_text=tokens,\n",
    "    top_pair_value=top_pair,\n",
    ")\n",
    "\n",
    "print(f\"{len(new_encoded_text)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "Do this iteratively, with a hyperparameter that determines the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class GPTTokenizer():\n",
    "    \"\"\"\n",
    "    Implements a simple Byte Pair Encoding (BPE) tokenizer.\n",
    "\n",
    "    This tokenizer iteratively replaces the most frequent adjacent byte pairs \n",
    "    with new byte values to form a more compact representation of the text.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoded_text: list[int],\n",
    "        num_iterations: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the tokenizer with encoded text and iteration count.\n",
    "\n",
    "        Args:\n",
    "            encoded_text (list[int]): A list of integers representing encoded text.\n",
    "            num_iterations (int): The number of iterations for byte pair merging.\n",
    "        \"\"\"\n",
    "        self.encoded_text = encoded_text\n",
    "        self.num_iterations = num_iterations\n",
    "        self.replace_byte_value = 256\n",
    "\n",
    "    def find_top_pair(self) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Finds the most frequently occurring adjacent byte pair.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: The most frequent byte pair.\n",
    "        \"\"\"\n",
    "        byte_pair_count = defaultdict(int)\n",
    "\n",
    "        for current_element, next_element in zip(self.encoded_text, self.encoded_text[1:]):\n",
    "            byte_pair = (current_element, next_element)\n",
    "            byte_pair_count[byte_pair] += 1\n",
    "\n",
    "        return max(byte_pair_count, key=byte_pair_count.get, default=(0, 0))\n",
    "\n",
    "    def merge_top_pair(\n",
    "        self,\n",
    "        top_pair: tuple[int, int],\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Replaces occurrences of a given byte pair in the encoded text.\n",
    "\n",
    "        This function scans the encoded text for consecutive occurrences of \n",
    "        `top_pair` and replaces them with `self.replace_byte_value`.\n",
    "\n",
    "        Args:\n",
    "            top_pair (tuple[int, int]): The byte pair to be merged.\n",
    "\n",
    "        Returns:\n",
    "            new_bytes (list[int]): The modified list with merged byte pairs.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        new_bytes = []\n",
    "\n",
    "        while i < len(self.encoded_text):\n",
    "            if i < len(self.encoded_text) - 1 and (self.encoded_text[i] == top_pair[0] and self.encoded_text[i+1] == top_pair[1]):\n",
    "                new_bytes.append(self.replace_byte_value)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_bytes.append(self.encoded_text[i])\n",
    "                i += 1\n",
    "        \n",
    "        self.encoded_text = new_bytes\n",
    "    \n",
    "    def tokenize(\n",
    "        self,\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Performs byte pair encoding for a fixed number of iterations.\n",
    "\n",
    "        This method iteratively finds and replaces the most frequent byte pairs \n",
    "        in the encoded text, assigning new byte values sequentially.\n",
    "\n",
    "        Returns:\n",
    "            (list[int]): The final encoded text after all iterations.\n",
    "        \"\"\"\n",
    "        for i in range(self.num_iterations):\n",
    "            top_pair = self.find_top_pair()\n",
    "            self.merge_top_pair(top_pair=top_pair)\n",
    "            self.replace_byte_value += 1\n",
    "            \n",
    "        return self.encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20\n",
    "\n",
    "tokenizer = GPTTokenizer(\n",
    "    encoded_text=tokens,\n",
    "    num_iterations=num_iterations\n",
    ")\n",
    "\n",
    "new_tokens = tokenizer.tokenize()\n",
    "\n",
    "# Perform simple check to check if this worked\n",
    "assert max(new_tokens) == 256 + num_iterations - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (110, 32) into a new token 256\n",
      "merging (101, 114) into a new token 257\n",
      "merging (116, 32) into a new token 258\n",
      "merging (101, 256) into a new token 259\n",
      "merging (107, 32) into a new token 260\n",
      "merging (101, 32) into a new token 261\n",
      "merging (115, 32) into a new token 262\n",
      "merging (101, 110) into a new token 263\n",
      "merging (105, 106) into a new token 264\n",
      "merging (44, 32) into a new token 265\n",
      "merging (101, 108) into a new token 266\n",
      "merging (97, 97) into a new token 267\n",
      "merging (105, 260) into a new token 268\n",
      "merging (257, 32) into a new token 269\n",
      "merging (105, 101) into a new token 270\n",
      "merging (105, 110) into a new token 271\n",
      "merging (101, 259) into a new token 272\n",
      "merging (100, 261) into a new token 273\n",
      "merging (46, 32) into a new token 274\n",
      "merging (111, 111) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    \n",
    "    return newids\n",
    "\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(110, 32): 256,\n",
       " (101, 114): 257,\n",
       " (116, 32): 258,\n",
       " (101, 256): 259,\n",
       " (107, 32): 260,\n",
       " (101, 32): 261,\n",
       " (115, 32): 262,\n",
       " (101, 110): 263,\n",
       " (105, 106): 264,\n",
       " (44, 32): 265,\n",
       " (101, 108): 266,\n",
       " (97, 97): 267,\n",
       " (105, 260): 268,\n",
       " (257, 32): 269,\n",
       " (105, 101): 270,\n",
       " (105, 110): 271,\n",
       " (101, 259): 272,\n",
       " (100, 261): 273,\n",
       " (46, 32): 274,\n",
       " (111, 111): 275}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My new solution\n",
    "\n",
    "- Added the printing and the merges variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "class GPTTokenizer():\n",
    "    \"\"\"\n",
    "    Implements a simple Byte Pair Encoding (BPE) tokenizer.\n",
    "\n",
    "    This tokenizer iteratively replaces the most frequent adjacent byte pairs \n",
    "    with new byte values to form a more compact representation of the text.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoded_text: list[int],\n",
    "        num_iterations: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the tokenizer with encoded text and iteration count.\n",
    "\n",
    "        Args:\n",
    "            encoded_text (list[int]): A list of integers representing encoded text.\n",
    "            num_iterations (int): The number of iterations for byte pair merging.\n",
    "        \"\"\"\n",
    "        self.encoded_text = encoded_text\n",
    "        self.num_iterations = num_iterations\n",
    "        self.replace_byte_value = 256\n",
    "        self.merges = {} # Will contain the replace byte-pairs and their replacement values.\n",
    "\n",
    "    def find_top_pair(self) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Finds the most frequently occurring adjacent byte pair.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: The most frequent byte pair.\n",
    "        \"\"\"\n",
    "        byte_pair_count = defaultdict(int)\n",
    "\n",
    "        for current_element, next_element in zip(self.encoded_text, self.encoded_text[1:]):\n",
    "            byte_pair = (current_element, next_element)\n",
    "            byte_pair_count[byte_pair] += 1\n",
    "\n",
    "        return max(byte_pair_count, key=byte_pair_count.get, default=(0, 0))\n",
    "\n",
    "    def merge_top_pair(\n",
    "        self,\n",
    "        top_pair: tuple[int, int],\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Replaces occurrences of a given byte pair in the encoded text.\n",
    "\n",
    "        This function scans the encoded text for consecutive occurrences of \n",
    "        `top_pair` and replaces them with `self.replace_byte_value`.\n",
    "\n",
    "        Args:\n",
    "            top_pair (tuple[int, int]): The byte pair to be merged.\n",
    "\n",
    "        Returns:\n",
    "            new_bytes (list[int]): The modified list with merged byte pairs.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        new_bytes = []\n",
    "\n",
    "        while i < len(self.encoded_text):\n",
    "            if i < len(self.encoded_text) - 1 and (self.encoded_text[i] == top_pair[0] and self.encoded_text[i+1] == top_pair[1]):\n",
    "                new_bytes.append(self.replace_byte_value)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_bytes.append(self.encoded_text[i])\n",
    "                i += 1\n",
    "        \n",
    "        self.encoded_text = new_bytes\n",
    "    \n",
    "    def tokenize(\n",
    "        self,\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Performs byte pair encoding for a fixed number of iterations.\n",
    "\n",
    "        This method iteratively finds and replaces the most frequent byte pairs \n",
    "        in the encoded text, assigning new byte values sequentially.\n",
    "\n",
    "        Returns:\n",
    "            (list[int]): The final encoded text after all iterations.\n",
    "        \"\"\"\n",
    "        print_width_1 = len(str(self.num_iterations-1))\n",
    "        print_width_2 = len(str(self.num_iterations + self.replace_byte_value -1))\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            top_pair = self.find_top_pair()\n",
    "\n",
    "            print(f\"{i=:0{print_width_1}}: Merging ({top_pair[0]:{print_width_2}}, {top_pair[1]:{print_width_2}}) into a new token {self.replace_byte_value}\")\n",
    "            self.merges[top_pair] = self.replace_byte_value\n",
    "\n",
    "            self.merge_top_pair(top_pair=top_pair)\n",
    "            self.replace_byte_value += 1\n",
    "            \n",
    "        return self.encoded_text, self.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=00: Merging (110,  32) into a new token 256\n",
      "i=01: Merging (101, 114) into a new token 257\n",
      "i=02: Merging (116,  32) into a new token 258\n",
      "i=03: Merging (101, 256) into a new token 259\n",
      "i=04: Merging (107,  32) into a new token 260\n",
      "i=05: Merging (101,  32) into a new token 261\n",
      "i=06: Merging (115,  32) into a new token 262\n",
      "i=07: Merging (101, 110) into a new token 263\n",
      "i=08: Merging (105, 106) into a new token 264\n",
      "i=09: Merging ( 44,  32) into a new token 265\n",
      "i=10: Merging (101, 108) into a new token 266\n",
      "i=11: Merging ( 97,  97) into a new token 267\n",
      "i=12: Merging (105, 260) into a new token 268\n",
      "i=13: Merging (257,  32) into a new token 269\n",
      "i=14: Merging (105, 101) into a new token 270\n",
      "i=15: Merging (105, 110) into a new token 271\n",
      "i=16: Merging (101, 259) into a new token 272\n",
      "i=17: Merging (100, 261) into a new token 273\n",
      "i=18: Merging ( 46,  32) into a new token 274\n",
      "i=19: Merging (111, 111) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 20\n",
    "\n",
    "tokenizer = GPTTokenizer(\n",
    "    encoded_text=tokens,\n",
    "    num_iterations=num_iterations\n",
    ")\n",
    "\n",
    "new_tokens, merges = tokenizer.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokens)=4379\n",
      "len(new_tokens)=3275\n",
      "\n",
      "Compression rate: 1.34\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(tokens)=}\")\n",
    "print(f\"{len(new_tokens)=}\")\n",
    "\n",
    "print(f\"\\nCompression rate: {len(tokens)/len(new_tokens):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Write the `encode` and `decode` methods between raw text (Unicode code point sequence) and a token sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My new solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result\n",
    "\n",
    "- Don't specify the number of iterations to perform, instead specify the desired vocabulary size and compute the number of iterations based on the number of unique byte values of your original text and the vocabulary size you will achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
